{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b17f5c15",
   "metadata": {},
   "source": [
    "# Short Story Generator using Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e1a553",
   "metadata": {},
   "source": [
    "Let's read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a20b2ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_path = \"dataset/train.csv\" \n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "train['text'] = train['text'].fillna('').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acc85de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows and columns in train dataset: (2119719, 1)\n",
      "\n",
      "Column names:\n",
      "Index(['text'], dtype='object')\n",
      "\n",
      "Data types of columns:\n",
      "text    object\n",
      "dtype: object\n",
      "\n",
      "Basic statistics of numerical columns:\n",
      "           text\n",
      "count   2119719\n",
      "unique  1799249\n",
      "top            \n",
      "freq        230\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows and columns in train dataset:\", train.shape)\n",
    "print(\"\\nColumn names:\")\n",
    "print(train.columns)\n",
    "print(\"\\nData types of columns:\")\n",
    "print(train.dtypes)\n",
    "print(\"\\nBasic statistics of numerical columns:\")\n",
    "print(train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c02f2748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One day, a little girl named Lily found a need...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Once upon a time, there was a little car named...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One day, a little fish named Fin was swimming ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Once upon a time, in a land full of trees, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Once upon a time, there was a little girl name...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  One day, a little girl named Lily found a need...\n",
       "1  Once upon a time, there was a little car named...\n",
       "2  One day, a little fish named Fin was swimming ...\n",
       "3  Once upon a time, in a land full of trees, the...\n",
       "4  Once upon a time, there was a little girl name..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91be1e9",
   "metadata": {},
   "source": [
    "# Trim Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1252031",
   "metadata": {},
   "source": [
    "The original dataset is very large. We trim it to be able to run it on local computer. Feel free to comment this line or change your desired dataset size based on your configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a372a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = 100\n",
    "train = train[:dataset_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba79ac83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a60d356",
   "metadata": {},
   "source": [
    "Make a tokenizer to build set of tokens from the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99b87020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "# Tokenize train data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train['text'])\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e2f92875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1491"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3259f0",
   "metadata": {},
   "source": [
    "Now we need to convert our text based inputs to numerical inputs. texts_to_squences converts text input to the numerical vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08d4f6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input sequences\n",
    "input_sequences = []\n",
    "for line in train['text']:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "82822f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example input:\n",
      "John and Sarah were playing together in their backyard when they found a piece of metal. It was shiny and reflective and they couldn't wait to show their parents. \n",
      "\n",
      "John asked Sarah, \"What should we do with the metal?\"\n",
      "\n",
      "Sarah thought for a moment, then said, \"Let's take it to Mommy and Daddy!\" With that, they ran off excitedly, ready to surprise their parents. \n",
      "\n",
      "They raced into the house, and shouted, \"Mommy, Daddy! Look what we found!\" \n",
      "\n",
      "Their parents were very surprised and asked, \"Where did you find this piece of metal?\" \n",
      "\n",
      "John and Sarah were so proud of their discovery, and couldn't wait to tell the story. They recounted that they found the metal outside in the backyard and it was so shiny and reflective. \n",
      "\n",
      "Their parents smiled, and said, \"Well, why don't you two take it around the neighbourhood and see if you can return it to its rightful owner. If nobody takes it, you two can keep it!\". \n",
      "\n",
      "John and Sarah were so cheerful and excited about the prospect of helping find the true owner of the metal, that they grabbed it and set off, ready to call on their neighbours.\n",
      "<------------------------------------->\n",
      "\n",
      "\n",
      "Converted vector:\n",
      "[596, 2, 260, 40, 81, 48, 10, 91, 724, 45, 9, 65, 3, 461, 19, 357, 6, 5, 388, 2, 948, 2, 9, 156, 251, 4, 252, 91, 406, 596, 67, 260, 98, 287, 115, 118, 16, 1, 357, 260, 96, 31, 3, 329, 52, 14, 131, 142, 6, 4, 108, 2, 219, 16, 17, 9, 82, 166, 922, 362, 4, 355, 91, 406, 9, 747, 160, 1, 141, 2, 475, 108, 219, 195, 98, 115, 65, 91, 406, 40, 30, 889, 2, 67, 447, 97, 21, 174, 87, 461, 19, 357, 596, 2, 260, 40, 13, 210, 19, 91, 1481, 2, 156, 251, 4, 846, 1, 339, 9, 1482, 17, 9, 65, 1, 357, 140, 10, 1, 724, 2, 6, 5, 13, 388, 2, 948, 91, 406, 44, 2, 14, 690, 215, 150, 21, 186, 142, 6, 68, 1, 1483, 2, 153, 175, 21, 55, 1484, 6, 4, 258, 1485, 949, 175, 883, 1486, 6, 21, 186, 55, 384, 6, 596, 2, 260, 40, 13, 1487, 2, 106, 126, 1, 1488, 19, 421, 174, 1, 668, 949, 19, 1, 357, 17, 9, 482, 6, 2, 1489, 166, 362, 4, 735, 34, 91, 1490]\n"
     ]
    }
   ],
   "source": [
    "print(\"Example input:\")\n",
    "print(line)\n",
    "print(\"<------------------------------------->\\n\\n\")\n",
    "print(\"Converted vector:\")\n",
    "print(tokenizer.texts_to_sequences([line])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25d304a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14700"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ce263f",
   "metadata": {},
   "source": [
    "The input text files includes stories with different lengths. We convert them to the same size input sequences using pad_sequences function. We use the length of longest story for finding the padding size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1857cf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences\n",
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5f7da3",
   "metadata": {},
   "source": [
    "There are infinite ways to define predictor labels. For example one can get first i words of a particular text as an input and the i+1 word as the label. We use the entire text except the last one as an input and the last word as a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b4aba75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Create predictors and label\n",
    "predictors, label = input_sequences[:, :-1],input_sequences[:, -1]\n",
    "label = np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6eef4841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14700, 211)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc9b6c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eaf8ac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(Bidirectional(LSTM(150)))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3df5166e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 211, 100)          149100    \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirecti  (None, 300)               301200    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1491)              448791    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 899091 (3.43 MB)\n",
      "Trainable params: 899091 (3.43 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ebe431c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "460/460 [==============================] - 636s 1s/step - loss: 5.9186 - accuracy: 0.0689\n",
      "Epoch 2/100\n",
      "460/460 [==============================] - 626s 1s/step - loss: 5.2878 - accuracy: 0.1137\n",
      "Epoch 3/100\n",
      "460/460 [==============================] - 631s 1s/step - loss: 4.7095 - accuracy: 0.1783\n",
      "Epoch 4/100\n",
      "460/460 [==============================] - 609s 1s/step - loss: 4.2156 - accuracy: 0.2143\n",
      "Epoch 5/100\n",
      "460/460 [==============================] - 610s 1s/step - loss: 3.8192 - accuracy: 0.2495\n",
      "Epoch 6/100\n",
      "460/460 [==============================] - 612s 1s/step - loss: 3.4718 - accuracy: 0.2806\n",
      "Epoch 7/100\n",
      "460/460 [==============================] - 615s 1s/step - loss: 3.1624 - accuracy: 0.3134\n",
      "Epoch 8/100\n",
      "460/460 [==============================] - 613s 1s/step - loss: 2.8747 - accuracy: 0.3565\n",
      "Epoch 9/100\n",
      "460/460 [==============================] - 613s 1s/step - loss: 2.6117 - accuracy: 0.4024\n",
      "Epoch 10/100\n",
      "460/460 [==============================] - 605s 1s/step - loss: 2.3640 - accuracy: 0.4484\n",
      "Epoch 11/100\n",
      "460/460 [==============================] - 605s 1s/step - loss: 2.1333 - accuracy: 0.5002\n",
      "Epoch 12/100\n",
      "460/460 [==============================] - 605s 1s/step - loss: 1.9202 - accuracy: 0.5541\n",
      "Epoch 13/100\n",
      "460/460 [==============================] - 607s 1s/step - loss: 1.7253 - accuracy: 0.6044\n",
      "Epoch 14/100\n",
      "460/460 [==============================] - 604s 1s/step - loss: 1.5480 - accuracy: 0.6446\n",
      "Epoch 15/100\n",
      "460/460 [==============================] - 606s 1s/step - loss: 1.3787 - accuracy: 0.6952\n",
      "Epoch 16/100\n",
      "460/460 [==============================] - 602s 1s/step - loss: 1.2289 - accuracy: 0.7343\n",
      "Epoch 17/100\n",
      "460/460 [==============================] - 604s 1s/step - loss: 1.0934 - accuracy: 0.7723\n",
      "Epoch 18/100\n",
      "460/460 [==============================] - 606s 1s/step - loss: 0.9698 - accuracy: 0.8029\n",
      "Epoch 19/100\n",
      "460/460 [==============================] - 607s 1s/step - loss: 0.8527 - accuracy: 0.8338\n",
      "Epoch 20/100\n",
      "460/460 [==============================] - 611s 1s/step - loss: 0.7431 - accuracy: 0.8632\n",
      "Epoch 21/100\n",
      "460/460 [==============================] - 609s 1s/step - loss: 0.6519 - accuracy: 0.8876\n",
      "Epoch 22/100\n",
      "460/460 [==============================] - 614s 1s/step - loss: 0.5647 - accuracy: 0.9126\n",
      "Epoch 23/100\n",
      "460/460 [==============================] - 612s 1s/step - loss: 0.4874 - accuracy: 0.9303\n",
      "Epoch 24/100\n",
      "460/460 [==============================] - 615s 1s/step - loss: 0.4203 - accuracy: 0.9473\n",
      "Epoch 25/100\n",
      "460/460 [==============================] - 615s 1s/step - loss: 0.3645 - accuracy: 0.9580\n",
      "Epoch 26/100\n",
      "460/460 [==============================] - 620s 1s/step - loss: 0.3118 - accuracy: 0.9667\n",
      "Epoch 27/100\n",
      "460/460 [==============================] - 617s 1s/step - loss: 0.2705 - accuracy: 0.9729\n",
      "Epoch 28/100\n",
      "460/460 [==============================] - 619s 1s/step - loss: 0.2303 - accuracy: 0.9792\n",
      "Epoch 29/100\n",
      "460/460 [==============================] - 619s 1s/step - loss: 0.2008 - accuracy: 0.9829\n",
      "Epoch 30/100\n",
      "460/460 [==============================] - 618s 1s/step - loss: 0.1753 - accuracy: 0.9844\n",
      "Epoch 31/100\n",
      "460/460 [==============================] - 619s 1s/step - loss: 0.1510 - accuracy: 0.9856\n",
      "Epoch 32/100\n",
      "460/460 [==============================] - 621s 1s/step - loss: 0.1380 - accuracy: 0.9857\n",
      "Epoch 33/100\n",
      "460/460 [==============================] - 622s 1s/step - loss: 0.1230 - accuracy: 0.9865\n",
      "Epoch 34/100\n",
      "460/460 [==============================] - 620s 1s/step - loss: 0.1114 - accuracy: 0.9863\n",
      "Epoch 35/100\n",
      "460/460 [==============================] - 623s 1s/step - loss: 0.1459 - accuracy: 0.9815\n",
      "Epoch 36/100\n",
      "460/460 [==============================] - 625s 1s/step - loss: 0.1251 - accuracy: 0.9845\n",
      "Epoch 37/100\n",
      "460/460 [==============================] - 624s 1s/step - loss: 0.0831 - accuracy: 0.9878\n",
      "Epoch 38/100\n",
      "460/460 [==============================] - 623s 1s/step - loss: 0.0685 - accuracy: 0.9882\n",
      "Epoch 39/100\n",
      "460/460 [==============================] - 625s 1s/step - loss: 0.0637 - accuracy: 0.9882\n",
      "Epoch 40/100\n",
      "460/460 [==============================] - 628s 1s/step - loss: 0.0615 - accuracy: 0.9878\n",
      "Epoch 41/100\n",
      "460/460 [==============================] - 629s 1s/step - loss: 0.0598 - accuracy: 0.9881\n",
      "Epoch 42/100\n",
      "460/460 [==============================] - 630s 1s/step - loss: 0.2132 - accuracy: 0.9458\n",
      "Epoch 43/100\n",
      "460/460 [==============================] - 632s 1s/step - loss: 0.2703 - accuracy: 0.9380\n",
      "Epoch 44/100\n",
      "460/460 [==============================] - 633s 1s/step - loss: 0.0800 - accuracy: 0.9879\n",
      "Epoch 45/100\n",
      "460/460 [==============================] - 634s 1s/step - loss: 0.0595 - accuracy: 0.9882\n",
      "Epoch 46/100\n",
      "460/460 [==============================] - 635s 1s/step - loss: 0.0533 - accuracy: 0.9888\n",
      "Epoch 47/100\n",
      "460/460 [==============================] - 636s 1s/step - loss: 0.0499 - accuracy: 0.9888\n",
      "Epoch 48/100\n",
      "460/460 [==============================] - 638s 1s/step - loss: 0.0503 - accuracy: 0.9880\n",
      "Epoch 49/100\n",
      "460/460 [==============================] - 639s 1s/step - loss: 0.0484 - accuracy: 0.9889\n",
      "Epoch 50/100\n",
      "460/460 [==============================] - 638s 1s/step - loss: 0.0478 - accuracy: 0.9885\n",
      "Epoch 51/100\n",
      "460/460 [==============================] - 637s 1s/step - loss: 0.0498 - accuracy: 0.9879\n",
      "Epoch 52/100\n",
      "460/460 [==============================] - 639s 1s/step - loss: 0.0548 - accuracy: 0.9872\n",
      "Epoch 53/100\n",
      "460/460 [==============================] - 640s 1s/step - loss: 0.3561 - accuracy: 0.9015\n",
      "Epoch 54/100\n",
      "460/460 [==============================] - 640s 1s/step - loss: 0.1253 - accuracy: 0.9748\n",
      "Epoch 55/100\n",
      "460/460 [==============================] - 641s 1s/step - loss: 0.0601 - accuracy: 0.9889\n",
      "Epoch 56/100\n",
      "460/460 [==============================] - 642s 1s/step - loss: 0.0496 - accuracy: 0.9890\n",
      "Epoch 57/100\n",
      "460/460 [==============================] - 643s 1s/step - loss: 0.0469 - accuracy: 0.9889\n",
      "Epoch 58/100\n",
      "460/460 [==============================] - 643s 1s/step - loss: 0.0446 - accuracy: 0.9888\n",
      "Epoch 59/100\n",
      "460/460 [==============================] - 647s 1s/step - loss: 0.0445 - accuracy: 0.9886\n",
      "Epoch 60/100\n",
      "460/460 [==============================] - 647s 1s/step - loss: 0.0442 - accuracy: 0.9883\n",
      "Epoch 61/100\n",
      "460/460 [==============================] - 647s 1s/step - loss: 0.0436 - accuracy: 0.9879\n",
      "Epoch 62/100\n",
      "460/460 [==============================] - 648s 1s/step - loss: 0.0422 - accuracy: 0.9883\n",
      "Epoch 63/100\n",
      "460/460 [==============================] - 650s 1s/step - loss: 0.0440 - accuracy: 0.9879\n",
      "Epoch 64/100\n",
      "460/460 [==============================] - 652s 1s/step - loss: 0.0849 - accuracy: 0.9777\n",
      "Epoch 65/100\n",
      "460/460 [==============================] - 651s 1s/step - loss: 0.3550 - accuracy: 0.8984\n",
      "Epoch 66/100\n",
      "460/460 [==============================] - 653s 1s/step - loss: 0.0898 - accuracy: 0.9826\n",
      "Epoch 67/100\n",
      "460/460 [==============================] - 652s 1s/step - loss: 0.0521 - accuracy: 0.9885\n",
      "Epoch 68/100\n",
      "460/460 [==============================] - 655s 1s/step - loss: 0.0445 - accuracy: 0.9890\n",
      "Epoch 69/100\n",
      "460/460 [==============================] - 654s 1s/step - loss: 0.0430 - accuracy: 0.9883\n",
      "Epoch 70/100\n",
      "460/460 [==============================] - 656s 1s/step - loss: 0.0417 - accuracy: 0.9886\n",
      "Epoch 71/100\n",
      "460/460 [==============================] - 658s 1s/step - loss: 0.0411 - accuracy: 0.9886\n",
      "Epoch 72/100\n",
      "460/460 [==============================] - 659s 1s/step - loss: 0.0404 - accuracy: 0.9883\n",
      "Epoch 73/100\n",
      "460/460 [==============================] - 657s 1s/step - loss: 0.0397 - accuracy: 0.9886\n",
      "Epoch 74/100\n",
      "460/460 [==============================] - 661s 1s/step - loss: 0.0395 - accuracy: 0.9887\n",
      "Epoch 75/100\n",
      "460/460 [==============================] - 658s 1s/step - loss: 0.0396 - accuracy: 0.9888\n",
      "Epoch 76/100\n",
      "460/460 [==============================] - 660s 1s/step - loss: 0.0389 - accuracy: 0.9888\n",
      "Epoch 77/100\n",
      "460/460 [==============================] - 660s 1s/step - loss: 0.0408 - accuracy: 0.9879\n",
      "Epoch 78/100\n",
      "460/460 [==============================] - 662s 1s/step - loss: 0.3360 - accuracy: 0.9023\n",
      "Epoch 79/100\n",
      "460/460 [==============================] - 659s 1s/step - loss: 0.1316 - accuracy: 0.9677\n",
      "Epoch 80/100\n",
      "460/460 [==============================] - 661s 1s/step - loss: 0.0561 - accuracy: 0.9880\n",
      "Epoch 81/100\n",
      "460/460 [==============================] - 660s 1s/step - loss: 0.0439 - accuracy: 0.9888\n",
      "Epoch 82/100\n",
      "460/460 [==============================] - 664s 1s/step - loss: 0.0407 - accuracy: 0.9890\n",
      "Epoch 83/100\n",
      "460/460 [==============================] - 667s 1s/step - loss: 0.0407 - accuracy: 0.9880\n",
      "Epoch 84/100\n",
      "460/460 [==============================] - 662s 1s/step - loss: 0.0394 - accuracy: 0.9892\n",
      "Epoch 85/100\n",
      "460/460 [==============================] - 663s 1s/step - loss: 0.0386 - accuracy: 0.9888\n",
      "Epoch 86/100\n",
      "460/460 [==============================] - 662s 1s/step - loss: 0.0391 - accuracy: 0.9884\n",
      "Epoch 87/100\n",
      "460/460 [==============================] - 665s 1s/step - loss: 0.0386 - accuracy: 0.9883\n",
      "Epoch 88/100\n",
      "460/460 [==============================] - 667s 1s/step - loss: 0.0379 - accuracy: 0.9886\n",
      "Epoch 89/100\n",
      "460/460 [==============================] - 665s 1s/step - loss: 0.0376 - accuracy: 0.9889\n",
      "Epoch 90/100\n",
      "460/460 [==============================] - 664s 1s/step - loss: 0.0374 - accuracy: 0.9884\n",
      "Epoch 91/100\n",
      "460/460 [==============================] - 663s 1s/step - loss: 0.0390 - accuracy: 0.9887\n",
      "Epoch 92/100\n",
      "460/460 [==============================] - 666s 1s/step - loss: 0.3995 - accuracy: 0.8806\n",
      "Epoch 93/100\n",
      "460/460 [==============================] - 663s 1s/step - loss: 0.1010 - accuracy: 0.9757\n",
      "Epoch 94/100\n",
      "460/460 [==============================] - 668s 1s/step - loss: 0.0522 - accuracy: 0.9878\n",
      "Epoch 95/100\n",
      "460/460 [==============================] - 667s 1s/step - loss: 0.0425 - accuracy: 0.9880\n",
      "Epoch 96/100\n",
      "460/460 [==============================] - 667s 1s/step - loss: 0.0401 - accuracy: 0.9888\n",
      "Epoch 97/100\n",
      "460/460 [==============================] - 666s 1s/step - loss: 0.0395 - accuracy: 0.9880\n",
      "Epoch 98/100\n",
      "460/460 [==============================] - 669s 1s/step - loss: 0.0384 - accuracy: 0.9888\n",
      "Epoch 99/100\n",
      "460/460 [==============================] - 667s 1s/step - loss: 0.0379 - accuracy: 0.9891\n",
      "Epoch 100/100\n",
      "460/460 [==============================] - 668s 1s/step - loss: 0.0370 - accuracy: 0.9888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x239354f8880>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(predictors, label, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ff06cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, model, max_sequence_len, tokenizer):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
    "        predicted_probs = model.predict(token_list, verbose=0)\n",
    "        predicted = np.argmax(predicted_probs)\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8b612ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "girl dog a girl named mia went for a walk she saw a big scary house it had a tall door and small windows mia was brave so she went inside the house in the house mia saw a birdcage inside the birdcage there was a little bird the bird was sad it wanted to fly and be free mia wanted to help the bird mia opened the birdcage door the bird flew out and was happy it was not scary anymore mia and the bird were friends they played and had fun all day and they decided to play hide and\n"
     ]
    }
   ],
   "source": [
    "# Keywords for text generation\n",
    "keywords = [\"girl\", \"dog\"]\n",
    "seed_text = ' '.join(keywords)  # Seed text with keywords\n",
    "\n",
    "# Generate text based on keywords\n",
    "generated_text = generate_text(seed_text, 100, model, max_sequence_len, tokenizer)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c081c1",
   "metadata": {},
   "source": [
    "# See impact of train data\n",
    "\n",
    "Let's find the input sequences which include \"mia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cc2c0df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_texts = []\n",
    "target_words = [\"Mia\", \"mia\"]\n",
    "for line in train['text']:\n",
    "    for w in target_words:\n",
    "        if w in line:\n",
    "            similar_texts.append(line)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c8575689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(similar_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "eaf0c15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ):\n",
      " One day, a girl named Mia went for a walk. She saw a big, scary house. It had a tall door and small windows. Mia was brave, so she went inside the house.\n",
      "\n",
      "In the house, Mia saw a birdcage. Inside the birdcage, there was a little bird. The bird was sad. It wanted to fly and be free. Mia wanted to help the bird.\n",
      "\n",
      "Mia opened the birdcage door. The bird flew out and was happy. It was not scary anymore. Mia and the bird were friends. They played and had fun all day.\n",
      "1 ):\n",
      " Once there was a little girl called Mia who loved to jump. Everywhere she went, she jumped. When walking to school, she would jump on the sidewalk. At the park, she would jump into the sandbox.\n",
      "\n",
      "One day Mia was at the supermarket and she saw something unusual. She saw a lawyer. Mia had never seen a lawyer before so it made her very curious. She wanted to know what a lawyer did and why he was so dressed up. So, Mia jumped right up to the lawyer and asked him.\n",
      "\n",
      "The lawyer was very confused. He had never seen a little girl so eager to talk to him. He tried to explain but Mia kept on jumping and interrupting.\n",
      "\n",
      "Soon enough the store manager got involved. He explained to Mia that it was not appropriate to engage with strangers and it was wrong to interrupt people when they were talking.\n",
      "\n",
      "Mia was very sorry for her behaviour and decided to never do something like this again. She had learned her lesson that it is important to be respectful to all strangers.\n"
     ]
    }
   ],
   "source": [
    "for i, txt in enumerate(similar_texts):\n",
    "    print(i,\"):\\n\", txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571a7196",
   "metadata": {},
   "source": [
    "### Due to the limited size of the training dataset, the generated story exhibits significant similarity with the training samples, notably featuring the word 'mia'. This issue can be mitigated by utilizing a larger training dataset. If system memory allows, let's proceed with the larger dataset to enhance the model's diversity and reduce overfitting. :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84348b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
